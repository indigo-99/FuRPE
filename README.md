# EXMODEL
Official code of FuRPE: Learning Full-body Reconstruction from Part Experts.
![image]((https://github.com/indigo-99/exmodel/pipelinefig.jpg)
FuRPE is based on distilling 3 part experts' knowledge (body, hand, and face) and finetuning the EXPOSE model.
(https://github.com/vchoutas/expose)

## USER GUIDANCE
### Training
We provide two training methods: the standard version [mytrain.py], and the multi-loss version [mytrain_multiloss.py].

1. [mytrain.py]  command (linux) :<br>     ```CUDA_VISIBLE_DEVICES=GPU_ID(eg: 0/1) nohup python mytrain.py --exp-cfg=data/config.yaml >log.train 2>&1 &```<br>
      - You can set whether to use feature distilization of 3 part (body, hand, and face) or not in the top of the code.<br>
      - --exp-cfg sets the configurations of the model, the important ones included are listed below: <br>
          - the training epoches
          - batch_size
          - saving path and frequency for checkpoints
          - the weights of each composition of the training loss
          - etc.
      - The training log will be recorded in log.train.
    
2. [mytrain_multiloss.py]  command (linux) :<br> ```CUDA_VISIBLE_DEVICES=GPU_ID(eg: 0/1) nohup python mytrain_multiloss.py --exp-cfg=data/config.yaml >log.train 2>&1 &```<br>
      - The same as the first one EXCEPT:<br>
        Add the idea of multi-task-learning (https://github.com/yaringal/multi-task-learning-example) into training, let the 3 part weights of body, hand and face to be trained together with the model.

3. [mytrain_ema.py]  command (linux) :<br> ```CUDA_VISIBLE_DEVICES=GPU_ID(eg: 0/1) nohup python mytrain_ema.py --exp-cfg=data/config.yaml >log.train 2>&1 &```<br>
      - The same as the first one EXCEPT:<br>
        Add the idea of EMA (https://github.com/yichen928/STRL) into training, using the self-supervision loss.

### Evaluation
We provide evaluation on EHF whole-body indoor dataset and another outdoor 3DPW testset (only body labels).<br>
In addition, part specific evaluation on the hand and head sub-networks are also provided.

1. Whole-body evaluation on EHF:  <br>```CUDA_VISIBLE_DEVICES=GPU_ID(eg: 0/1) python inference.py --exp-cfg=data/config.yaml   --datasets=ehf  --show=False    --output-folder eval_output  --save-mesh=False  --save-params=False  --save-vis=False```

2. Body evaluation on 3DPW testset: <br>```CUDA_VISIBLE_DEVICES=GPU_ID(eg: 0/1) python inference.py --exp-cfg=data/config.yaml   --datasets=threedpw  --show=False    --output-folder eval_output  --save-mesh=False  --save-params=False  --save-vis=False```<br>
   Before running the commond, the code of expose/evaluation.py need to be changed in line 723~729 to change the J_regressor from SMPL-X to SMPL because 3DPW only contains ground truth in SMPL formats.
   
3. Hand evaluation on FREIHAND testset: <br>```CUDA_VISIBLE_DEVICES=GPU_ID(eg: 0/1) python inference_freihand.py --exp-cfg=data/config.yaml   --datasets=ehf  --show=False    --output-folder eval_output  --save-mesh=False  --save-params=False  --save-vis=False```<br>
   <br>```cd /data/panyuqing/freihand```<br>
   <br>```python eval.py  /data/panyuqing/freihand/evaluation  /data/panyuqing/freihand/evaluation/output```<br>
   The freihand code can be referenced and installed according to: https://github.com/lmb-freiburg/freihand

4. Head evaluation on NoW testset: <br>```CUDA_VISIBLE_DEVICES=GPU_ID(eg: 0/1) python inference_nowface.py --exp-cfg=data/config.yaml   --datasets=ehf  --show=False    --output-folder eval_output  --save-mesh=False  --save-params=False  --save-vis=False```<br>
   The dataset authors also don't publish the ground truth of the testset, so the code only generate predicted results. If evaluation metrics are needed, you can submit the result to their mailbox. (https://ringnet.is.tue.mpg.de/download.php)

### Demo
The demo videos can be generated by running:<br>
   ```CUDA_VISIBLE_DEVICES=GPU_ID(eg: 0/1) nohup python mydemo.py  --image-folder test_video_or_imagedir_path  --exp-cfg=data/config.yaml  --output-folder output_demo   --save-vis=True >log.mydemo 2>&1 &```<br>
   You can set the demo's input videos or image folders by --image-folder, and the output one by --output-folder.<br>
   The config.yaml is only used to locate the checkpoint path.

## MODEL DOWNLOAD
URL: https://pan.baidu.com/s/1TCP-UFrUwsYHhJ0oHYCNlg 
pwd: pupb 


## EXPERT SYSTEM
Our methods can use the expert system to generate pseudo labeled data of whole-body pose (body, hand, head pose, shape, expression), which can be used in model training to improve model's performance.<br>
The expert system consists of 3 experts:<br>
1. The body expert: SPIN (https://github.com/nkolot/SPIN)
2. The hand expert: FrankMocap (https://github.com/facebookresearch/frankmocap)
3. The head expert: DECA (https://github.com/YadiraF/DECA)<br>
We use the models of the 3 experts published by their authors to generate part labels, then integrate them to get whole-body labels with 3D vertices for training.<br>

Before sending images to the expert system, we use mediapipe, an open source human part detection model, to crop body, hand, and head images. (https://google.github.io/mediapipe/)<br>
